<!doctype html>
<html>

<head>
  <meta charset="utf-8">
  <title>Artificial Intelligence in Zero Sum Games</title>
  <link rel="stylesheet" href="/assets/css/styles.css">

  <script type="text/javascript" src="/assets/js/main.js"></script>
  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>
  <link href="https://fonts.googleapis.com/css?family=Titillium+Web&display=swap" rel="stylesheet">
  <link rel='stylesheet' href='https://unpkg.com/emoji.css/dist/emoji.min.css'>
  <link rel="stylesheet" href="/assets/css/fontello.css">
  
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.11.2/css/all.css">
  <link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap.min.css"
    integrity="sha384-ggOyR0iXCbMQv3Xipma34MD+dH/1fQ784/j6cY/iJTQUOhcWr7x9JvoRxT2MZw1T" crossorigin="anonymous">
</head>

<body>
  <section class="tabs">
    <h1>AUGUSTE LEHUGER</h1>
    <h3>Personnal Projects Blog</h3>
    <div class="tabs-container">
        
        <a class="tab" href="/about" >
            About 
        </a>
        
        <a class="tab" href="/" >
            Projects 
        </a>
        
        <a class="tab" href="/contact" >
            Contact 
        </a>
        
    </div>
</section>

  <div class="row">
      <div class=" col-3">
        <section class="profile-container">
    <div class="row">
        <div class="col-md-8 offset-2">
            <img src="/assets/docs/profile.png" alt="PP" class="img-fluid">
        </div>
    </div>
    <div class="row">
        <h4><a class="" href="/about.html"> Auguste Lehuger </a> </h2>
    </div>
    
    <p class="tagline"> Machine Learning Developer</p>
    <i class="icon-map-marker"></i>
    <p> <span class="ec ec-round-pushpin"></span> London, UK </p>
    <div class="row">   
        <a href="https://github.com/alehuger"><i class="icon-github-circled"></i></a>
        <a href="https://linkedin.com/in/alehuger"><i class="icon-linkedin-squared"></i></a>
    </div>
    
    <p>&copy; 2019</p>
</section>


      </div>
      <div class=" col-9">
          <div class="content-container">
            <div class="post-page">

    <h1>Artificial Intelligence in Zero Sum Games</h1>

    <div class="post-meta">
        <ul class="post-categories">
            
            <li>AI</li>
            
            <li>Monte Carlo Search</li>
            
            <li>Game Theory</li>
            
            <li>AlphaGo</li>
            
        </ul>
        <div class="post-date">20 Mar 2018
        </div>
    </div>

    <div class="row post-header">
        <div class="col-9">
            <p style="font-size: 20em;"><p>An agent is trained to play a two players board game with complete information such as Tic-Tac-Toe, Othello or Go. 
Given the rules of the game, the algorithm behind the agent will have to train only through self-play without any human knowledge.
This work is based on <code class="highlighter-rouge"> AlphaZero’s algorithm</code>, DeepMind’s latest breakthrough in the field.</p>
</p>
        </div>
        <div class="col-3 ">
            <img src="/assets/docs/posts/images/ai2.png" alt="Picture not loaded" class="img-fluid">
        </div>
    </div>

    <div class="row content">
        <div class="col-11">
            
<h2>Intro </h2>

<p>
This algorithm comes in three parts: defining the rules of the Game, the Monte-Carlo Search Tree (MCTS) methods and a Neural Network that deduces the action to choose from a given state of the board. For the sake of simplicity, we will take the example of the Tic-Tac-Toe to illustrate the concept showed here. 
</p>

<div class="row offset-3">
<figure>
  <img src="assets/docs/posts/images/ttt-illustrated.png" alt="Not Showing" />
  <figcaption> Three different states of the Tic-Tac-Toe </figcaption>
</figure>
</div>

<h4> Neural Network Purpose </h4>

<p>
  The objective of our algorithm is to find the optimal move for any given state of the board. 
  In order for any machine learning algorithm to discover what makes a good state and, consequently, a good action it requires data from numerous games.
</p>

<h4> Tree Search </h4>

<h2> Game Setting </h2>

<p>
The first step of the project is to create the pipeline for a simple board game such as Tic-Tac-Toe. Then, we will adapt it to more complex games.
</p>

<h2> Tree Search Simulates Ground Truths From Scratch </h2>

<p>Our tree search is composed of several phases :</p>

<h4> Rollout  </h4>

<p>
We take for root a node <b> S </b> and perform a simulation : we choose the action <b> A </b> that maximizes a utility function <b> U(S, A) </b> defined by : 

$$ U(S, A) = Q(S, A) + c_{puct} . P(S, A) . \displaystyle{\frac{\sqrt{\sum_b N(S, b)}}{1 + N(S, A)}} $$
where
<ul>
    <li> <b>N(S, A) is the visit counter</b> : action A has been taken N(S, A) times from node S </li>
    <li> <b>Q(S, A) is the expected reward </b> from playing action A at node S </li>
    <li> <b> <em> P(S, A) </em> is the initial policy </b> given by the neural network </li>
    <li> <b> c<sub>puct</sub> is a hyperparameter </b> tuning the degree of exploration. </li>
</ul>

<p>
After playing action <b> A </b>, we update visit counter <b> N(S, A) </b> and we get to the new node <b> S’ </b>. s’ is either newly visited, in which case we simply add this node to the tree, and initialize N(s’, b) and Q(s’, b) to 0 for all actions b while calling the neural network for the policy P(s’, .). In the other case, it has already been visited and then we call the function rollout recursively on s’ until a new node or a terminal state is reached. In that case we back propagate to update all expected rewards Q(s, a)inthepath.
The number of rollouts called at each node is a hyperparam- eter of our model. Once all rollouts have been performed and all values updated, we have produced examples e.g. samples which will be turned into policies π(s) by just taking the empirical mean of the visit counter N(s, .) and normalize it. During a real game against a player, the agent will choose as an action a∗ = argmaxa(π(s)).
</p>


<p>

</p>

<h2> Neural Networks Learning </h2>

<p>

</p>
</p>

        </div>
    </div>

    <div class="row post-header">
        
        <p>
                <a href="/assets/docs/posts/pdfs/OthelloAI.pdf"> For more information, check the paper available here</a>
        </p>
        
    </div>

</div>
          </div>
      </div>
    </div>
</body>

</html>